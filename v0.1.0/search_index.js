var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API","title":"API Reference","text":"This page documents the main functions and types provided by OdeMMHPlanner.","category":"section"},{"location":"api/#Sampling","page":"API","title":"Sampling","text":"Functions and types related to Bayesian learning of system dynamics and latent state trajectories using the Marginal Metropolis–Hastings (MMH) sampler.","category":"section"},{"location":"api/#Analysis-and-Diagnostics","page":"API","title":"Analysis and Diagnostics","text":"Utilities for analyzing and diagnosing the MCMC chains produced by the MMH sampler.","category":"section"},{"location":"api/#Optimal-Control","page":"API","title":"Optimal Control","text":"Functions for formulating and solving the scenario-based optimal control problem using posterior samples obtained from MMH.","category":"section"},{"location":"api/#OdeMMHPlanner.MMH_sample","page":"API","title":"OdeMMHPlanner.MMH_sample","text":"MMH sample\n\nFields:\n\ntheta: parameters\nx_t: state at current time step t=0\nx_init: initial state of the training trajectory, i.e., state at t=-T; this is used to adapt the proposal distribution in the staged sampler\n\n\n\n\n\n","category":"type"},{"location":"api/#OdeMMHPlanner.ODE_MMH","page":"API","title":"OdeMMHPlanner.ODE_MMH","text":"ODE_MMH(u_t::Function, t_m::AbstractVector{<:AbstractFloat}, y::AbstractMatrix{<:AbstractFloat}, t_span::Tuple{Float64,Float64}, K::Int, K_b::Int, k_d::Int, f_theta!::Function, g_theta!::Function, log_pdf_w_theta::Function, log_pdf_theta::Function, theta_0::AbstractVector{<:AbstractFloat}, log_pdf_x_init::Function, x_init_0::AbstractVector{<:AbstractFloat}, propose_z::Function, log_proposal_ratio_z::Function; ODE_solver=RK4(), ODE_solver_opts=(dt=0.1, adaptive=false), print_progress=true)\n\nRun marginal Metropolis-Hastings (MMH) with ODE-integrated latent trajectory to obtain samples theta x(t=0)^1K from the joint parameter and state posterior distribution p(theta x(t=0) mid mathbbD=u(t) y_1M).\n\nArguments\n\nu_t: input trajectory; function of time t\nt_m: time points of the output measurements, must lie within t_span\ny: output measurements\nt_span: timespan of the training trajectory\nK: number of models/scenarios to be sampled\nK_b: length of the burn in period\nk_d: number of models/scenarios to be skipped to decrease correlation (thinning)\nf_theta!: dynamics function parametrized by theta (mutating); has inputs (dotx theta x u t)\ng_theta!: measurement function parametrized by theta (mutating); has inputs (g theta x u t)\nlog_pdf_w_theta: function that returns the logarithm of the probability density function of the measurement noise parametrized by theta; has inputs (theta w)\nlog_pdf_theta: function that returns the logarithm of the probability density function of theta (prior); has input (theta)\ntheta_0: theta used to initialize the MMH sampler\nlog_pdf_x_init: function that returns the logarithm of probability density function of x(t=-T) (prior); has input x(t=-T)\nx_init_0: x(t=-T) used to initialize the MMH sampler\npropose_z: function that proposes new parameters z = theta x(t=-T) (proposal distribution); has input (z = theta x(t=-T)])\nlog_proposal_ratio_z: function that computes the logarithm of the ratio of proposal densities for z; has input arguments (z z)\nODE_solver: ODE solver algorithm to use (we use RK4 as default as this is also used in the optimal control formulation)\nODE_solver_opts: ODE solver setting\nprint_progress: if true, the progress is printed (default: true)\n\nReturns\n\nMMH_samples: MMH samples\nacceptance_ratio: acceptance ratio of the MMH sampler\nruntime: runtime of the sampling process\n\n\n\n\n\n","category":"function"},{"location":"api/#OdeMMHPlanner.staged_ODE_MMH","page":"API","title":"OdeMMHPlanner.staged_ODE_MMH","text":"staged_ODE_MMH(u_t::Function, t_m::AbstractVector{<:AbstractFloat}, y::AbstractMatrix{<:AbstractFloat}, t_span::Tuple{Float64,Float64}, K::Int, K_b::Int, k_d::Int, f_theta!::Function, g_theta!::Function, log_pdf_w_theta::Function, log_pdf_theta::Function, theta_0::AbstractVector{<:AbstractFloat}, log_pdf_x_init::Function, x_init_0::AbstractVector{<:AbstractFloat}, proposal_z_cov_0::AbstractMatrix{<:AbstractFloat}, M_chunk::Int, K_stage::Int, alpha::Union{AbstractFloat,AbstractVector{<:AbstractFloat}}; regularizer::Float64=1e-8, ODE_solver=RK4(), ODE_solver_opts=(dt=0.1, adaptive=false), print_progress=true)\n\nRun marginal Metropolis-Hastings (MMH) with ODE-integrated latent trajectory with incremental data and adaptive proposal to obtain samples theta x(t=0)^1K from the joint parameter and state posterior distribution p(theta x(t=0) mid mathbbD=u(t) y_1M). The number of data points used in the likelihood computation is gradually increased by a fixed chunk size. At each stage, the MMH sampler is run on the current data subset, and the proposal distribution is adapted based on the empirical covariance of the collected samples.\n\nArguments\n\nu_t: input trajectory; function of time t\nt_m: time points of the output measurements, must lie within t_span\ny: output measurements\nt_span: timespan of the training trajectory\nK: number of models/scenarios to be sampled\nK_b: length of the burn in period\nk_d: number of models/scenarios to be skipped to decrease correlation (thinning)\nf_theta!: dynamics function parametrized by theta (mutating); has inputs (dotx theta x u t)\ng_theta!: measurement function parametrized by theta (mutating); has inputs (g theta x u t)\nlog_pdf_w_theta: function that returns the logarithm of the probability density function of the measurement noise parametrized by theta; has inputs (theta w)\nlog_pdf_theta: function that returns the logarithm of the probability density function of theta (prior); has input (theta)\ntheta_0: theta used to initialize the MMH sampler\nlog_pdf_x_init: function that returns the logarithm of probability density function of x(t=-T) (prior); has input x(t=-T)\nx_init_0: x(t=-T) used to initialize the MMH sampler\nproposal_z_cov_0: initial covariance of the multivariate normal proposal distribution for z = theta x(t=-T) (e.g., covariance of the prior)\nM_chunk: number of data points added at each stage\nK_stage: number of samples per stage\nalpha: proposal scaling factor (scalar or vector; if a vector its i‐th element is used at stage i)\nregularizer: small constant added to the diagonal of the proposal covariance matrix to ensure positive definiteness (default: 1e-8)\nODE_solver: ODE solver algorithm to use (we use RK4 as default as this is also used in the optimal control formulation)\nODE_solver_opts: ODE solver setting\nprint_progress: if true, the progress is printed (default: true)\n\nReturns\n\nMMH_samples: final samples from full-data posterior\nacceptance_ratio: vector containing the acceptance ratio of each stage\nruntime: total runtime of the staged sampling process\n\n\n\n\n\n","category":"function"},{"location":"api/#OdeMMHPlanner.compute_autocorrelation","page":"API","title":"OdeMMHPlanner.compute_autocorrelation","text":"compute_autocorrelation(MMH_samples::Vector{MMH_sample}; max_lag::Int=100)\n\nCompute the autocorrelation function (ACF) of the MMH samples.\n\nArguments\n\nMMH_samples: MMH samples\nmax_lag: maximum lag at which to calculate the ACF\n\nReturns\n\nautocorrelation: matrix containing the ACF for each variable\n\n\n\n\n\n","category":"function"},{"location":"api/#OdeMMHPlanner.compute_ess","page":"API","title":"OdeMMHPlanner.compute_ess","text":"compute_ess(MMH_samples::Vector{MMH_sample}; max_lag::Int=100)\n\nCompute the effective sample size (ESS) for each parameter and initial state.\n\nArguments\n\nMMH_samples: MMH samples\nmax_lag: maximum lag for autocorrelation estimation\n\nReturns\n\ness: vector of ESS estimates for all variables\n\n\n\n\n\n","category":"function"},{"location":"api/#OdeMMHPlanner.compute_gelman_rubin","page":"API","title":"OdeMMHPlanner.compute_gelman_rubin","text":"compute_gelman_rubin(MMH_chains::Vector{Vector{MMH_sample}})\n\nCompute the Gelman–Rubin statistic for each parameter and initial state from a vector of MMH chains.\n\nArguments\n\nMMH_chains: vector of chains, where each chain is a vector of MMH samples\n\nReturns\n\nR_hat: vector of R̂ values, one for each variable\n\n\n\n\n\n","category":"function"},{"location":"api/#OdeMMHPlanner.solve_MMH_OCP","page":"API","title":"OdeMMHPlanner.solve_MMH_OCP","text":"solve_MMH_OCP(MMH_samples::Vector{MMH_sample}, n_u::Int, f_theta::Function, g_theta::Function, H::Float64, N::Int, c::Function, c_f::Function, h_scenario::Function, h_u::Function; U_init=nothing, MMH_samples_pre_solve=nothing, K_warmup=0, solver_opts=nothing, rk4_step_size=0.1, print_progress=true)\n\nSolve the continous time scenario optimal control problem of the following form:\n\nmin_u(cdot) J_sc(u(cdot)) = frac1K sum_k=1^K  c_f(x^k(H)) + int_0^H c(u(t) x^k(t) t) dt\n\nsubject to: \n\nbeginaligned\nforall t in 0H forall k  in mathbbN_leq K \nx^k(t) = Phi(t theta^k x^k(0) u(cdot)) \nh_scenario(u(t) x^k(t) t) leq 0\nendaligned\n\nHere, Phi(t theta^k x^k(0) u(cdot)) denotes the solution at time t of the ODE dotx(t) = f_theta(x(t) u(t)) with initial condition x(0) = x^k(0) and parameter theta under the input trajectory u(cdot).\n\nArguments\n\nMMH_samples: MMH samples\nn_u: number of control inputs\nf_theta: dynamics function parametrized by theta (non mutating); has inputs (theta x u t)\ng_theta: measurement function parametrized by theta (non mutating); has inputs (theta x u t)\nH: horizon of the OCP (as time)\nN: number of discretization steps in the horizon\nc: function with input arguments (u(t) x^k(t) t) that returns the running cost of scenario k at time t\nc_f: function with input argument x^k(H) that returns the terminal cost of scenario k\nh_scenario: function with input arguments (u(t) x^k(t) t) that returns the constraint vector for scenario k at time t; a feasible solution must satisfy h_mathrmscenario leq 0 for all scenarios and all discretization points\nh_u: function with input arguments (u(t) t) that returns the constraint vector for the control inputs; a feasible solution satisfy h_u leq 0 at all discretization points\nU_init: initial guess for the input trajectory - either a n_u x N array, a function with input argument t, or nothing (default: nothing)\nMMH_samples_pre_solve: if provided, an initial guess for the input trajectory is obtained by solving an OCP with the samples in MMH_samples_pre_solve only\nK_warmup: if K_warmup > 0 and MMH_samples_pre_solve is provided, an initial guess for the the input trajectory is obtained in a two stage process: first, an OCP with only K_warmup samples from MMH_samples_pre_solve is solved and then an OCP with all samples in MMH_samples_pre_solve\nsolver_opts: SolverOptions struct containing options of the solver\nrk4_step_size: step size of the RK4 integrator used to simulate the system dynamics\nprint_progress: if set to true, the progress is printed\n\nReturns\n\nU_opt: piecewise constant optimal input trajectory, array of dimension n_u x N\nX_opt: states at the discretization points for all scenarios, array of dimension n_x x N x K\nt_grid: time grid of the discretization points, array of dimension N + 1\nJ_sc_opt: optimal cost J_sc\nsolve_successful: true if the optimization was successful, false otherwise\niterations: number of iterations of the solver\nruntime: runtime of the optimization\n\n\n\n\n\n","category":"function"},{"location":"experiments/#experiments","page":"Experiments","title":"Experiments","text":"This repository contains three main experiment scripts used in the simulation study presented in Section “Simulation” of the paper. The experiments evaluate the proposed framework on a glucose regulation task based on the Bergman minimal model under infrequent and noisy glucose measurements.\n\nAll experiment scripts are located in the experiments/ directory.","category":"section"},{"location":"experiments/#Sampler-Tuning-and-Diagnostics","page":"Experiments","title":"Sampler Tuning and Diagnostics","text":"Script: experiments/sampler_tuning.jl\n\nThis experiment runs the MMH sampler for a large number of iterations without thinning and analyzes the resulting Markov chain to assess sampling performance. In particular, it computes diagnostics such as autocorrelation functions (ACFs), which are used to determine an appropriate thinning interval for subsequent experiments.\n\nThe outputs produced by this script are used to generate Figure 1 in the paper.\n\nFor a detailed explanation of the tuning and diagnostic procedure, see Inference and Sampler Tuning.","category":"section"},{"location":"experiments/#Scenario-Based-Optimal-Control-(Single-Run)","page":"Experiments","title":"Scenario-Based Optimal Control (Single Run)","text":"Script: experiments/optimal_control.jl\n\nThis experiment demonstrates the full pipeline on a single representative run:\n\nBayesian inference of model parameters and latent state trajectories from infrequent measurements (MMH sampler with ODE integration),\nscenario-based optimal control using posterior samples,\nevaluation by simulating the true system forward and comparing against baselines (e.g., nominal + EKF).\n\nThe outputs produced by this script are used to generate Figure 2 in the paper.\n\nFor a conceptual explanation of the scenario OCP setup, see Optimal Control.","category":"section"},{"location":"experiments/#Monte-Carlo-Study","page":"Experiments","title":"Monte Carlo Study","text":"Folder: experiments/monte_carlo/\n\nThis study repeats the single-run pipeline across 100 independent simulation runs, sampling the true model parameters and initial conditions from the prior distributions. It is used to assess robustness and performance statistically.\n\nThe aggregated results of this study are summarized in Table 2 of the paper.\n\nDetailed instructions, including information on running the experiments using SLURM job scripts, are provided in experiments/monte_carlo/README.md.","category":"section"},{"location":"examples/sampling/#sampling","page":"Inference and Sampler Tuning","title":"Inference and Sampler Tuning","text":"This example demonstrates how to perform Bayesian inference with the Marginal Metropolis–Hastings (MMH) sampler with ODE-integrated latent trajectories, and how to tune the sampler for reliable posterior exploration. Proper tuning is essential for efficient mixing and for obtaining posterior samples that are suitable for downstream planning and control.\n\nThe complete, executable script corresponding to this section is available at experiments/sampler_tuning.jl.","category":"section"},{"location":"examples/sampling/#Problem-Setting","page":"Inference and Sampler Tuning","title":"Problem Setting","text":"We consider a continuous-time dynamical system with unknown parameters and latent states, observed through infrequent and noisy output measurements. In this example, the system is a glucose–insulin model for a patient with type-1 diabetes.\n\nAssumptions in the example:\n\nThe structure of the dynamics is known.\nA subset of model parameters and the initial latent state are unknown.\nOnly noisy measurements of the output are available (no full state measurements).\nMeasurements are available only at discrete, irregular time instants.\n\nThe goal of inference is to obtain samples from the posterior distribution over unknown parameters and latent state trajectories that are consistent with the observed input–output data.","category":"section"},{"location":"examples/sampling/#Setup-and-Data-Generation","page":"Inference and Sampler Tuning","title":"Setup and Data Generation","text":"The script defines the glucose–insulin dynamics and generates synthetic data to mimic a realistic learning scenario. The training set consists of sparse glucose measurements over a 12-hour window; additional measurements are retained as a test set for posterior validation.\n\nThe full model definition is available in the script.","category":"section"},{"location":"examples/sampling/#ODE-Solver-Configuration","page":"Inference and Sampler Tuning","title":"ODE Solver Configuration","text":"The MMH sampler embeds a numerical ODE solver to propagate candidate dynamics between measurement times. Any solver from DifferentialEquations.jl can be used.\n\nFor control: the fixed-step solver RK4() is recommended for consistency with the optimal control formulation.\nFor long trajectories or fine resolutions: variable-step solvers with appropriate tolerances can significantly reduce runtime.\n\nusing DifferentialEquations\n\nrk4_step_size = 0.5 # step size for RK4 solver\nODE_solver = RK4() # ODE solver algorithm\nODE_solver_opts = (dt=rk4_step_size, adaptive=false) # ODE solver options","category":"section"},{"location":"examples/sampling/#Prior-Specification","page":"Inference and Sampler Tuning","title":"Prior Specification","text":"Prior selection is a critical part of tuning: it incorporates domain knowledge and restricts the region of parameter space explored by the sampler.\n\nOverly broad priors can lead to slow convergence and poor mixing.\nOverly restrictive priors can bias the inference and prevent recovery of plausible models.\n\nIn this example:\n\nUnknown parameters are modeled on a log scale to enforce positivity.\nLog-normal priors are chosen such that most probability mass lies within physically plausible ranges reported in the literature.\n\nconst theta_mean = [\n    -4.26,      # log(p2)\n    -13.27,     # log(p3)\n    -1.66       # log(n)\n]\n\nconst theta_var = [\n    0.18^2,\n    0.28^2,\n    0.23^2\n]\n\nA Gaussian prior is also assumed for the initial latent state, centered near a basal equilibrium with moderate variance.","category":"section"},{"location":"examples/sampling/#Running-the-MMH-Sampler","page":"Inference and Sampler Tuning","title":"Running the MMH Sampler","text":"We employ a staged MMH strategy, which incrementally increases the amount of data used during sampling. This helps the sampler locate high-probability regions of the posterior before long runs are used for uncertainty quantification.","category":"section"},{"location":"examples/sampling/#Key-tuning-parameters","page":"Inference and Sampler Tuning","title":"Key tuning parameters","text":"Staged refinement:\n\nM_chunk: number of measurements added per stage\nK_stage: number of samples drawn per stage\nalpha: proposal covariance scaling (target acceptance rate ≈ 25%)\n\nFinal sampling:\n\nK: total number of samples\nk_d: thinning factor (used to reduce autocorrelation)\n\nMMH_samples, acceptance_ratio, runtime = staged_ODE_MMH(\n    # data, model, and prior definitions\n)","category":"section"},{"location":"examples/sampling/#Practical-tuning-guidance:","page":"Inference and Sampler Tuning","title":"Practical tuning guidance:","text":"If the acceptance ratio is far from ≈25%, adjust alpha (increase it if acceptance is too high; decrease it if acceptance is too low).\nIf chains mix poorly (high autocorrelation, sticky traces), increase K_stage and/or reduce M_chunk.","category":"section"},{"location":"examples/sampling/#Diagnostics-and-Sampler-Assessment","page":"Inference and Sampler Tuning","title":"Diagnostics and Sampler Assessment","text":"After sampling, it is essential to verify that the chain has mixed well and provides reliable posterior samples.","category":"section"},{"location":"examples/sampling/#Trace-Plots","page":"Inference and Sampler Tuning","title":"Trace Plots","text":"Trace plots should appear approximately stationary after burn-in, without long-term trends, and should show reasonable jump sizes. Persistent drift or long plateaus typically indicate insufficient mixing or overly aggressive proposals.","category":"section"},{"location":"examples/sampling/#Autocorrelation-and-ESS","page":"Inference and Sampler Tuning","title":"Autocorrelation and ESS","text":"The autocorrelation function (ACF) reveals the dependence between consecutive samples, while the effective sample size (ESS) summarizes how many effectively independent samples are available.\n\nautocorrelation = compute_autocorrelation(MMH_samples; max_lag=50)\ness = compute_ess(MMH_samples; max_lag=100)\n\n(Image: autocorrelation)\n\nA well-mixed chain exhibits rapidly decaying autocorrelation across all parameters and latent states. If autocorrelation remains high at large lags, this indicates poor mixing and suggests adjusting the proposal scaling (alpha), increasing the number of samples per stage (K_stage), or reducing the amount of data added per stage (M_chunk).\n\nA key tuning objective is to maximize ESS per unit time, while obtaining approximately independent samples.","category":"section"},{"location":"examples/sampling/#Posterior-Validation","page":"Inference and Sampler Tuning","title":"Posterior Validation","text":"Posterior samples are validated by simulating the system forward and comparing predictions to held-out test data.\n\n(Image: prediction)\n\nThe posterior mean should track the test trajectory reasonably well, while the prediction band reflects uncertainty arising from sparse and noisy measurements.\n\nQualitative indicators of a well-calibrated posterior include:\n\ngood agreement between posterior mean and test data,\nuncertainty bands that neither collapse prematurely nor grow excessively wide.\n\nIf these conditions are not met, revisit prior specification and sampler tuning before proceeding to optimal control.\n\nIf they are satisfied, the inferred posterior models are suitable for downstream control tasks.","category":"section"},{"location":"examples/control/#optimal-control","page":"Optimal Control","title":"Optimal Control","text":"Once a posterior over the system dynamics and latent states has been obtained via the MMH sampler, it can be used to compute control inputs that explicitly account for model uncertainty.\n\nThis example demonstrates how to formulate and solve a scenario-based optimal control problem (OCP) using posterior samples learned from infrequent output measurements.\n\nThe full implementation can be found in experiments/optimal_control.jl.","category":"section"},{"location":"examples/control/#Problem-Setting","page":"Optimal Control","title":"Problem Setting","text":"We consider an optimal control problem in which the system dynamics are uncertain and only partially observed during training.\n\nThe objective is to compute a control input (u(t)) (e.g., insulin infusion) that\n\nminimizes a performance cost (e.g., deviation from a reference trajectory),\nrespects safety constraints (e.g., bounds on glucose levels),\nremains robust with respect to uncertainty in the learned dynamics.\n\nRather than optimizing with respect to a single nominal model, the control input is optimized jointly across multiple scenarios, where each scenario corresponds to one posterior sample obtained from the MMH sampler.\n\nThe result is a single control trajectory that performs well across a representative set of system realizations consistent with the available measurements and prior information.","category":"section"},{"location":"examples/control/#Cost-Function-and-Constraints","page":"Optimal Control","title":"Cost Function and Constraints","text":"The OCP is specified through standard Julia functions defining the running cost, terminal cost, and constraints.\n\n# Running cost:\n# Penalize deviation from the reference glucose level and control effort.\nc(u, x, t) = W_G * (x[1] - G_REF)^2 + W_U * (u[1] - U_BASAL)^2 \n\n# Terminal cost:\n# Encourage convergence to the reference at the end of the horizon.\nc_f(x) = W_Gf * (x[1] - G_REF)^2\n\n# State constraints:\n# Glucose must remain within safety bounds.\n# Constraints are formulated as h(u, x, t) ≤ 0.\nh_scenario(u, x, t) = [\n    x[1] - G_MAX;   # x[1] ≤ G_MAX\n    G_MIN - x[1]    # x[1] ≥ G_MIN\n]\n\nThese definitions are independent of the uncertainty representation and can be adapted to other systems and applications.","category":"section"},{"location":"examples/control/#Scenario-based-optimal-control","page":"Optimal Control","title":"Scenario-based optimal control","text":"Given a set of posterior samples obtained from the MMH sampler, the scenario OCP is solved by discretizing the dynamics and optimizing jointly across all scenarios.\n\n# Prediction horizon and discretization\nH = 6 * 60.0       # 6 hours\nN = Int(H / 0.5)   # discretization points\n\n# Solve the scenario-based OCP\nU_opt, X_opt, t_grid, J_opt = solve_MMH_OCP(\n    MMH_samples,        # posterior samples (scenarios)\n    n_u,                # number of control inputs\n    f_theta, g_theta,   # dynamics and measurement models\n    H, N,               # horizon and grid\n    c, c_f,             # costs\n    h_scenario,         # state constraints\n    h_u                 # input constraints\n)\n\nInternally, solve_MMH_OCP propagates each scenario forward in time and enforces shared control inputs across all scenarios, yielding a control policy that explicitly accounts for uncertainty in the learned model.","category":"section"},{"location":"examples/control/#Evaluation-and-comparison","page":"Optimal Control","title":"Evaluation and comparison","text":"The optimized control trajectory is evaluated by simulating the true system forward in time.\n\n(Image: optimal_control)\n\nIn optimal_control.jl, the proposed uncertainty-aware controller is compared against several baselines:\n\nNominal OCP, using a point estimate of the dynamics and an EKF state estimate,\nNo control, serving as a lower-performance baseline,\nSimple bolus control, representing a heuristic therapy rule.\n\nThe results typically show that the MMH-based controller behaves more conservatively, respecting safety constraints even for unfavorable parameter realizations, and achieves lower overall cost due to improved prediction under uncertainty.\n\nIn contrast, nominal controllers may violate constraints as a result of overconfidence in a single model estimate.","category":"section"},{"location":"#OdeMMHPlanner.jl","page":"Home","title":"OdeMMHPlanner.jl","text":"Welcome to the documentation for OdeMMHPlanner.jl. This package provides a framework for uncertainty-aware learning and planning in dynamical systems with unknown dynamics and infrequent output measurements.\n\nOdeMMHPlanner implements the method described in:\n\nLearning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control Robert Lefringhausen, Theodor Springer, Sandra Hirche arXiv:2512.08013 (2025) https://arxiv.org/abs/2512.08013","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"The package targets control problems in which the system dynamics are unknown and the state is only partially observed through infrequent and noisy output measurements. Rather than identifying a single nominal model, OdeMMHPlanner explicitly represents uncertainty over both the system dynamics and the latent state trajectory.\n\nThe approach follows a Bayesian workflow:\n\nLearning: A Marginal Metropolis–Hastings (MMH) sampler, equipped with a numerical ODE solver, is used to sample from the posterior distribution over unknown dynamics and latent state trajectories, conditioned on infrequent input–output measurements.\nPlanning: The resulting posterior samples are propagated through the system dynamics and used to formulate a scenario-based optimal control problem, yielding control inputs that explicitly account for model uncertainty.\n\nBy propagating uncertainty from system identification into the control design, the framework enables principled uncertainty quantification and safer decision-making compared to point-estimate-based approaches.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"This package is not registered in the General registry. Clone the repository and instantiate the environment locally:\n\ngit clone https://github.com/TUM-ITR/ode-mmh-planner.git\ncd ode-mmh-planner\njulia --project=. -e 'using Pkg; Pkg.instantiate()'","category":"section"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"The following sections provide a structured entry point into the package:\n\nInference and Sampler Tuning: Introduces the Bayesian learning problem underlying OdeMMHPlanner. This section explains how unknown dynamics and latent state trajectories are inferred from infrequent input–output data using the Marginal Metropolis–Hastings sampler, and how to tune and diagnose the sampler to obtain reliable posterior samples.\nOptimal Control: Demonstrates how the inferred posterior models are used to formulate and solve a scenario-based optimal control problem that explicitly accounts for model uncertainty.","category":"section"},{"location":"#Reproducing-the-Experiments","page":"Home","title":"Reproducing the Experiments","text":"The numerical results reported in the paper are generated using the experiment scripts provided in the repository.   An overview of these experiments, including sampler diagnostics, single-run optimal control, and a Monte Carlo study, is given on the Experiments page.","category":"section"}]
}
